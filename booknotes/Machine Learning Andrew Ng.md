# video: [Stanford: Machine Learning by Andrew Ng](https://www.youtube.com/playlist?list=PLJ1-ciQ35nuiyL1PX6O4NdF5CjjaDdnVC)

## Blah
this is not a booknote, but a video notes, but whatever.

## notations

under link context:

    ?              # todos



## chapter 2:

###2.6 Gradient Descent Intuition:
Gradient Descent Properties:
* if learning rate is too high, it may not converge even deverge
* if learning rate is too low, it just may take many steps(or iterations) to finally get converged
* if learning rate if fixed, the descent rate still get smaller with each iteration towards bottom (local optimal)
* most the case: theta0 or theta1 often initialized with 0

###2.7 Gradient Descent For Linear Regression
includes:
* linear regression's cost functional always be a convex function (or a bowl shaped function), which has only one local optimal value.
* what is `Batch Gradient Descent` a type of descent use all training example to iterate.

###2.8 What's Next
# Links
* [? Derivative](https://en.wikipedia.org/wiki/Derivative)
